{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the normalized feature $x_2^{(2)}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  89, 7921,   96],\n",
       "       [  72, 5184,   74],\n",
       "       [  94, 8836,   87],\n",
       "       [  69, 4761,   78]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"\"\"89\t7921\t96\n",
    "72\t5184\t74\n",
    "94\t8836\t87\n",
    "69\t4761\t78\"\"\".split()\n",
    "\n",
    "ar = np.array(a, dtype=int).reshape((4, 3))\n",
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = ar[:, 1]\n",
    "res = (line - line.mean()) / (max(line) - min(line))\n",
    "round(res[1], 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You run gradient descent for 15 iterations with $\\alpha = 0.3$ and compute $J(\\theta)$ after each iteration. You find that the value of $J(\\theta)$ decreases quickly then levels off. Based on this, which of the following conclusions seems most plausible?\n",
    "\n",
    "\n",
    "\n",
    "- Rather than use the current value of $\\alpha$, it'd be more promising to try a smaller value of $\\alpha$ (say $\\alpha = 0.1$).\n",
    "\n",
    "\n",
    "- $\\alpha = 0.3$ is an effective choice of learning rate. + \n",
    "\n",
    "\n",
    "- Rather than use the current value of $\\alpha$, it'd be more promising to try a larger value of $\\alpha$ (say $\\alpha = 1.0$).\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have $m=14$ training examples with $n = 3$ features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is $\\theta = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot y$. For the given values of $m$ and $n$, what are the dimensions of $\\theta$, $X$, and $y$ in this equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = n +1  \n",
    "\n",
    "- $X: m \\times n$  \n",
    "- $X^T: n \\times m$  \n",
    "- $y: m \\times 1$  \n",
    "- $\\theta: n \\times 1$ \n",
    "\n",
    "$$X \\times \\theta = y => m \\times n \\cdot n \\times 1 = m \\times 1 $$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a dataset with m = 50 examples and n = 15 features for each example. You want to use multivariate linear regression to fit the parameters \\thetaÎ¸ to our data. Should you prefer gradient descent or the normal equation?\n",
    "\n",
    "\n",
    "\n",
    "- The normal equation, since it provides an efficient way to directly find the solution. + \n",
    "\n",
    "\n",
    "- The normal equation, since gradient descent might be unable to find the optimal $\\theta$.\n",
    "\n",
    "\n",
    "- Gradient descent, since it will always converge to the optimal $\\theta$.\n",
    "\n",
    "\n",
    "- Gradient descent, since $(X^TX)^{-1}$ will be very slow to compute in the normal equation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Which of the following are reasons for using feature scaling?\n",
    "\n",
    "- It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.\n",
    "\n",
    "\n",
    "- It speeds up gradient descent by making it require fewer iterations to get to a good solution. + \n",
    "\n",
    "\n",
    "- It prevents the matrix X^TX (used in the normal equation) from being non-invertable (singular/degenerate).\n",
    "\n",
    "\n",
    "- It is necessary to prevent the normal equation from getting stuck in local optima.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
